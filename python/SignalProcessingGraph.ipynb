{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Documentation : http://docs.scipy.org/doc/scipy/reference/sparse.html\n",
    "\"\"\"\n",
    "Sparse matrices can be used in arithmetic operations: they support addition, subtraction, multiplication, division, and matrix power.\n",
    "Advantages of the CSR format\n",
    "\n",
    "        efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n",
    "        efficient row slicing\n",
    "        fast matrix vector products\n",
    "Disadvantages of the CSR format\n",
    "\n",
    "        slow column slicing operations (consider CSC)\n",
    "        changes to the sparsity structure are expensive (consider LIL or DOK)\n",
    "\"\"\"\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import dia_matrix\n",
    "import scipy.sparse.linalg as spl\n",
    "\n",
    "import scipy as sc\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal processing on graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "First, we would like to thank John D Cook for its [short introduction](https://www.johndcook.com/blog/2016/02/09/fourier-transform-of-a-function-on-a-graph/) on Fourier series on graph, and of course [Pierre Vandergheynst](https://scholar.google.ch/citations?user=1p9NOFEAAAAJ&hl=fr), for its interesting courses and its contributions to this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier series\n",
    "\n",
    "Fourier series properties can be analyzed within various mathenatical framework. One of them is related to the Laplacian operator.\n",
    "\n",
    "We recall that the Laplacian operator, sometimes written $\\nabla \\cdot \\nabla$, $\\nabla^2$ or $\\Delta$, where $\\nabla$ can be written $\\left( \\frac{\\partial}{\\partial x_0}, \\frac{\\partial}{\\partial x_1}, \\dots \\frac{\\partial}{\\partial x_{n-1}} \\right)$ and the laplacian operator applied to a function $f$ reads $\\Delta f = \\sum_{i=0}^{n-1} \\frac{\\partial f}{\\partial x_i^2}$.\n",
    "\n",
    "It is interesting to notice that the trigonometric polynomial that defines the (separable) Fourier series basis elements are eigenfunctions for the laplacian operators over euclidean spaces:\n",
    "\n",
    "lets define $\\delta_{nF}(x) = e^{2 \\pi j nFx}$\n",
    "\n",
    "And\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\delta_{nF}}{\\partial x} (x) &= \\frac{\\partial  e^{2 \\pi j nFx}}{\\partial x} \\\\\n",
    "  &= 2 \\pi j nF e^{2 \\pi j nFx} \\\\\n",
    "\\end{align}\n",
    "\n",
    "And\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\delta_{nF}}{\\partial x^2} (x) &=  -(2\\pi nF)^2 e^{2 \\pi j nFx} \\\\\n",
    "  &= -(2\\pi nF)^2 \\delta_{nF}\\\\\n",
    "  &= \\lambda \\delta_{nF}\n",
    "\\end{align}\n",
    "\n",
    "Moreover, eigenfunctions of the laplacian can also be defined for functions on finite domains (Bessel functions for radially symmetric domain like disks and sphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of calculus on graphs\n",
    "\n",
    "### Definitions, operators\n",
    "Graphs are not euclidean spaces, lets define some calculus elements before jumping on more advanced stuff:\n",
    "\n",
    "| Name | ........................ Definition ........................ | Comment |\n",
    "|:----:|:----------:|:-------:|\n",
    "| Graph | $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ | |\n",
    "| Vertices | $\\mathcal{V} = {1,\\dots, n}$ | n is the cardinality of the vertices |\n",
    "| Vertex weights | $b_i > 0 \\forall i \\in \\mathcal{V}$| |\n",
    "| Edges | $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$ | |\n",
    "| Edge weights | $a_{ij} \\geq 0 \\quad \\forall \\quad (i,j) \\quad \\in \\mathcal{E}$ | |\n",
    "| Vertex field | $L^2(\\mathcal{V}) = {f : \\mathcal{V} \\rightarrow \\mathbb{R}^n}$ | Functional space of all $L^2$-integrable functions <br> sampled over the graph (1 real value per vertex) |\n",
    "| graph function | $f = (f_1, \\dots, f_n)$ | One element of $L^2(\\mathcal{V})$ presented above  |\n",
    "| Inner product | $\\langle f,g \\rangle_{L^2(\\mathcal{V})} = \\sum_{i \\in \\mathcal{V}} b_i f_i g_i$ | Allows to define a functional Hilbert <br> space of graph-based functions |\n",
    "| Gradient operator | $(\\nabla f)_{ij} = \\sqrt{a_{ij}}(f_i-f_j)$ | $\\nabla : L^2(\\mathcal{V}) \\rightarrow L^2(\\mathcal{E})$ |\n",
    "| Divergence operator | $(div F)_{i} = \\frac{1}{b_i} \\sum_{j:(i,j) \\in \\mathcal{E}} \\sqrt{a_{ij}}(F_{ji}-F_{ij})$ |$div : L^2(\\mathcal{E}) \\rightarrow L^2(\\mathcal{V})$ |\n",
    "| Gradient Adjoint |$\\nabla^{\\star}F = -div F$ | $\\langle F,\\nabla f \\rangle_{L^2(\\mathcal{E})} = \\langle \\nabla^{\\star}F, f \\rangle_{L^2(\\mathcal{V})} = \\langle -div F, f \\rangle_{L^2(\\mathcal{V})}$ |\n",
    "| Laplacian operator | $(\\Delta F)_{i} = \\frac{1}{b_i} \\sum_{j:(i,j) \\in \\mathcal{E}} a_{ij}(f_i-f_j)$ | $\\Delta : L^2(\\mathcal{V}) \\rightarrow L^2(\\mathcal{V})$, this one is the $D-A$ |\n",
    "| Weight matrix | $A = (a_{ij})$ | | |\n",
    "| Degree matrix | $D = diag(\\sum_{j\\neq i}a_{ij})$ | | |\n",
    "| Unnormalized Laplacian | $\\Delta = D - A$ | | |\n",
    "| Unnormalized Laplacian | $\\Delta = I - D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$ | | |\n",
    "| Random walk Laplacian | $\\Delta = I - D^{-1} A $ | | |\n",
    "\n",
    "Those definitions are coming from slides from Xavier Bresson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must notice, that we prefered the following notation for the divergence operator (although it is exactly the same thing):\n",
    "\n",
    "| Name | ............................. Definition ............................. | ...............Comment............... |\n",
    "|:----:|:----------:|:-------:|\n",
    "| Divergence operator | $(div F)_{i} = \\sum_{(k,l) \\in \\mathcal{E}} \\sqrt{a_{kl}}(\\delta_{i}(l)-\\delta_{i}(k)) F_{kl}$ |$div : L^2(\\mathcal{E}) \\rightarrow L^2(\\mathcal{V})$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define some utilities for testing on random/symmetric sparse matrices\n",
    "\n",
    "# define graph adjacency matrix as a random csr matrix\n",
    "def gen_random_spm(size):\n",
    "    return sparse.rand(size[0], size[1], density=0.9, format='csr', dtype=np.float32, random_state=None)\n",
    "\n",
    "def gen_random_graph(size):\n",
    "    A = gen_random_spm((size,size))\n",
    "    # Now make it symmetric\n",
    "    return A+A.T-sparse.diags(A.diagonal())\n",
    "\n",
    "# define graph adjacency matrix as a small symmetric csr matrix\n",
    "def gen_small_graph():\n",
    "    weight=[4]\n",
    "    rowIdx=[0]\n",
    "    colIdx=[1]\n",
    "    A = csr_matrix((np.array(weight), (np.array(rowIdx), np.array(colIdx))),dtype=np.float32, shape=(2, 2))\n",
    "    #Build a symmetric matrix\n",
    "    return A+A.T-sparse.diags(A.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define some operators, term by term and/or with a matrix:\n",
    "def grad(G,f):\n",
    "    \"\"\"-G is the graph adjacency matrix\n",
    "       -f is the function defined on this graph\n",
    "       Output has the dimension of the adjacency matrix\n",
    "       Elementwise version\n",
    "    \"\"\"\n",
    "    g = lil_matrix(G.shape)\n",
    "    #iterate over all edges, row_index is i, col_idx is j\n",
    "    for row_idx, col_idx, weight in zip(*G.nonzero(), G.data):\n",
    "        g[row_idx, col_idx] = np.sqrt(weight)*(f[row_idx]-f[col_idx])\n",
    "    return g\n",
    "\n",
    "# lets define some operators, term by term and/or with a matrix:\n",
    "def div(G,F,b=None):\n",
    "    \"\"\"-G is the graph adjacency matrix\n",
    "       -F is the gradient of function f on this graph\n",
    "       -b is the optional vertex_weight\n",
    "       Output is 1D and has its size equal to the cardinality of the vertices\n",
    "       Elementwise version\n",
    "    \"\"\"\n",
    "    # If b is not provided, assume all vertices have weight 1\n",
    "    if b is None:\n",
    "        v_weight = lambda idx: 1\n",
    "    else:\n",
    "        v_weight = lambda idx: 1/b[idx]\n",
    "        \n",
    "    def delta_i(i):\n",
    "        return lambda x: 1 if x==i else 0\n",
    "    \n",
    "    div = np.zeros(G.shape[0])\n",
    "    for row_idx in range(div.size):\n",
    "        d = 0\n",
    "        for g_row_idx, g_col_idx, weight in zip(*G.nonzero(), G.data):\n",
    "            #print('row_idx is {}, col_idx is {}, weight is {}'.format(g_row_idx, g_col_idx, weight))\n",
    "            #print('F is {}, delta+ is {}, delta- is {}'.format(F[g_row_idx, g_col_idx],\n",
    "            #                                                   delta_i(row_idx)(g_row_idx),\n",
    "            #                                                   delta_i(row_idx)(g_col_idx)))\n",
    "            d += np.sqrt(weight)*F[g_row_idx, g_col_idx]*(delta_i(row_idx)(g_col_idx)-delta_i(row_idx)(g_row_idx))\n",
    "            #d += np.sqrt(weight)*(F[row_idx, col_idx]-F[col_idx,row_idx])\n",
    "        div[row_idx] = d#v_weight(row_idx)*d\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define those operators as linear operators\n",
    "\n",
    "def get_edge_grad_matrix(G):\n",
    "    \"\"\"-G is the graph adjacency matrix\n",
    "    Returns the matrix that takes a function f, valued of the the network and returns the gradient\n",
    "    (as a linear index (0,0) (0,1) (0,2)... (1,0) (1,1) (1.2)...\n",
    "    \"\"\"\n",
    "    grad_mat = lil_matrix((np.prod(G.shape), G.shape[0]), dtype=np.float32)\n",
    "    #print('G shape is {} gradmat shape is {}'.format(G.shape,grad_mat.shape))\n",
    "    for i in range(G.shape[0]):\n",
    "        for j in range(G.shape[1]):\n",
    "            weight = np.sqrt(G[i,j])\n",
    "            grad_mat[i*G.shape[1]+j,i]=0\n",
    "            grad_mat[i*G.shape[1]+j,j]=0\n",
    "            grad_mat[i*G.shape[1]+j,i]+=weight\n",
    "            grad_mat[i*G.shape[1]+j,j]-=weight\n",
    "    return grad_mat\n",
    "\n",
    "def grad_with_matrix(G,f):\n",
    "    \"\"\"-G is the graph adjacency matrix\n",
    "       -f is the function defined on this graph\n",
    "       Output has the dimension of the adjacency matrix\n",
    "       Elementwise version\n",
    "    \"\"\"\n",
    "    return get_edge_grad_matrix(G).dot(csr_matrix(f.reshape(f.size,1))).tolil().reshape(G.shape)\n",
    "\n",
    "def div_with_matrix(G,F,b=None):\n",
    "    \"\"\"-G is the graph adjacency matrix\n",
    "       -F is the gradient of function f on this graph\n",
    "       -b is the optional vertex_weight\n",
    "       Output is 1D and has its size equal to the cardinality of the vertices\n",
    "       Elementwise version\n",
    "    \"\"\"\n",
    "    return (-get_edge_grad_matrix(G).T).dot(F.reshape((np.prod(G.shape),1))).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if both grad give the same result:\n",
    "size = 20\n",
    "G1 = gen_random_graph(size)\n",
    "#G1 = gen_small_graph()\n",
    "f1 = np.random.rand(size)\n",
    "grad1 = grad(G1, f1)\n",
    "gradm1 = grad_with_matrix(G1, f1)\n",
    "assert(np.allclose(grad1.todense(), gradm1.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following assumption, for a given linear operator A, and its transpose B, where we cannot explicitly obtain $A^t$ neither $B^t$:\n",
    "\n",
    "\\begin{align*}\n",
    "    xA^t A y &= x^tBAy \\\\\n",
    "    x^t \\cdot (A^tA y) &= x^t \\cdot (BAy) \\\\\n",
    "    (x^tA^t) \\cdot (A y) &= (x^tB) \\cdot (Ay) \\\\\n",
    "    (Ax)^t \\cdot (Ay) &= (B^tx) \\cdot (Ay)\n",
    "\\end{align*}\n",
    "\n",
    "By comparing $a = x^t \\cdot (BAy)$ and $b = (Ax)^t \\cdot (Ay)$ that both give a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if matrix grad and matrix div are transpose of each other (by definition they should...)\n",
    "f1 = np.random.rand(size)\n",
    "f2 = np.random.rand(size)\n",
    "a = f1.dot(-div_with_matrix(G1, grad_with_matrix(G1, f2)))\n",
    "b = grad_with_matrix(G1, f1).multiply(grad_with_matrix(G1, f2)).sum()\n",
    "assert(np.allclose(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if grad and div are transpose of each other\n",
    "f1 = np.random.rand(size)\n",
    "f2 = np.random.rand(size)\n",
    "\n",
    "a = f1.dot(-div(G1, grad(G1, f2)))\n",
    "b = grad(G1, f1).multiply(grad(G1, f2)).sum()\n",
    "assert(np.allclose(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying graph thanks to those operators\n",
    "\n",
    "It is interesting to notice that the concept of smoothness/regularity can be extended to graph structured data as well as:\n",
    "\n",
    "smoothness($f$) =\n",
    "\\begin{align}\n",
    "  & \\quad \\langle f, \\Delta f \\rangle_{L^2(\\mathcal{V})} \\\\\n",
    "  = &\\sum_{(i,j) \\in \\mathcal{E} \\; \\text{s.t} \\; i\\neq j} (f_i-f_j)^2 \\\\\n",
    "  \\geq & \\qquad 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier series on graph\n",
    "\n",
    "Following the definition seen in the previous section, we can see that on cane define eigenfunctions of laplacian operator on various domain including graph, for instance in our case, where we consider our laplacian operator $L$ as: $L=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ where $A$ is the adjacency matrix and $D$ is the degree matrix of the graph.\n",
    "\n",
    "Hopefully, in an undirected graph, $L$ is a symmetric positive semi definite matrix, hence is diagonalizable in an orthonornmal basis. A function that is defined by a set of values, one per node can then be analyzed over this graph, using eigenvectors of the graph ordered by the norm of their corresponding eigenvalues.\n",
    "\n",
    "On can then consider that projection of the function over the lower eigenvectors corresponds to low \"frequencies\", and conversely, the high \"frequencies\" are obtained when projecting the function over the highest eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import an image, and downsize it in order to make the problem numerically tractable\n",
    "im = sc.misc.imresize(sc.misc.ascent(),size=(128,128))\n",
    "im = im.astype(np.float32)\n",
    "plt.imshow(im, interpolation='nearest', cmap=cm.gray, vmin=0, vmax=255)\n",
    "figsize(8,8)\n",
    "plt.axis(\"off\")\n",
    "colorbar()       # displays the color bar close to the image\n",
    "title('Initial image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold: valued used to forbid disconnected nodes in the graph of the image\n",
    "thresh= 1e-4\n",
    "\n",
    "#Sigma of the gaussian distance metric\n",
    "sigma = 2\n",
    "sigmaSquare = sigma**2\n",
    "\n",
    "#Size of the stencil\n",
    "stenSize = 4\n",
    "\n",
    "#will contain the value of the edges\n",
    "data=[]\n",
    "rowIdx=[]\n",
    "colIdx=[]\n",
    "\n",
    "\"\"\"\n",
    "Now we are going to fill the graph matrix, neglecting small terms.\n",
    "Distance are actually computed twice, this can be optimized\n",
    "\"\"\"\n",
    "\n",
    "for idxI in range(im.shape[0]):\n",
    "  for idxJ in range(im.shape[1]):\n",
    "    curPixIdx=idxI+im.shape[0]*idxJ\n",
    "    for stenX in range(-stenSize,stenSize+1):\n",
    "      for stenY in range(-stenSize,stenSize+1):\n",
    "        stenIdxX = idxI+stenX\n",
    "        stenIdxY = idxJ+stenY\n",
    "        destPixIdx = stenIdxX+im.shape[0]*stenIdxY\n",
    "        if (stenIdxX>=0)and(stenIdxX<im.shape[0])and\\\n",
    "          (stenIdxY>=0)and(stenIdxY<im.shape[1])and\\\n",
    "          ((stenIdxX!=idxI)and(stenIdxY!=idxJ)):\n",
    "          dist = np.exp(-np.linalg.norm(im[idxI,idxJ]-im[stenIdxX,stenIdxY])**2/(2*sigmaSquare))\n",
    "          dist=max(thresh,dist)\n",
    "          data.append(dist)\n",
    "          rowIdx.append(curPixIdx)\n",
    "          colIdx.append(destPixIdx)\n",
    "\n",
    "\"\"\"\n",
    "   Practical implementation using scipy support for sparse matrices, The compressed sparse row (CSR) format\n",
    "   represents a matrix M by three (one-dimensional) arrays, that respectively contain:\n",
    "      -nonzero values\n",
    "      -the extents of rows\n",
    "      -column indices\n",
    "    This format allows fast row access and matrix-vector multiplications (Mx)\n",
    "\"\"\"\n",
    "A = csr_matrix((np.array(data), (np.array(rowIdx), np.array(colIdx))), dtype=np.float32)\n",
    "\n",
    "#Now the diagonal matrix\n",
    "D = dia_matrix((np.array(A.sum(axis=1)),0),shape=A.shape)\n",
    "\n",
    "#We can define the laplacian: L = D^-0.5 A D^-0.5\n",
    "L = D.power(-0.5)*A*D.power(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the eigendecomposition of the laplacian of the graph\n",
    "\"\"\"\n",
    "\n",
    "# Get the first 100 eigenvalues of L, a symmetric positive definite matrix\n",
    "# See http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigsh.html and\n",
    "# http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.linalg.svds.html\n",
    "[w,X] = spl.eigsh(L, k=100) #size is now [imsize,nbcomponent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the spectrum of the image, i.e its expression in the basis made of the first lapacian eigenvectors\n",
    "spectrum = np.abs(np.dot(X.T,im.reshape(im.size)))\n",
    "plt.plot(spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Norm of each eigenvector\n",
    "#print(\"Norm is \"+str(np.linalg.norm(X[:,0])))\n",
    "\n",
    "for i in np.arange(X.shape[1])[::-1]:\n",
    "  figure(i)\n",
    "  imshow(np.reshape(X[:,i],im.shape), interpolation='nearest', cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
